{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./PITdf.pkl','rb') as f:\n",
    "    PITdf = pickle.load(f)\n",
    "with open('./GREdf.pkl','rb') as f:\n",
    "    GREdf = pickle.load(f)\n",
    "with open('./JONdf.pkl','rb') as f:\n",
    "    JONdf = pickle.load(f)\n",
    "with open('./MGTdf.pkl','rb') as f:\n",
    "    MGTdf = pickle.load(f)\n",
    "with open('./WASdf.pkl','rb') as f:\n",
    "    WASdf = pickle.load(f)\n",
    "with open('./WHLdf.pkl','rb') as f:\n",
    "    WHLdf = pickle.load(f)\n",
    "with open('./PKSdf.pkl','rb') as f:\n",
    "    PKSdf = pickle.load(f)\n",
    "with open('./CBGdf.pkl','rb') as f:\n",
    "    CBGdf = pickle.load(f)\n",
    "with open('./STUdf.pkl','rb') as f:\n",
    "    STUdf = pickle.load(f)\n",
    "with open('./NPHdf.pkl','rb') as f:\n",
    "    NPHdf = pickle.load(f)\n",
    "with open('./ELVdf.pkl','rb') as f:\n",
    "    ELVdf = pickle.load(f)\n",
    "with open('./YGTdf.pkl','rb') as f:\n",
    "    YGTdf = pickle.load(f)\n",
    "with open('./NCSdf.pkl','rb') as f:\n",
    "    NCSdf = pickle.load(f)\n",
    "with open('./BUTdf.pkl','rb') as f:\n",
    "    BUTdf = pickle.load(f)\n",
    "with open('./KITdf.pkl','rb') as f:\n",
    "    KITdf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Month  Day  Hour  Minute  DNI  Wind Speed  Wind Direction    wind_x  \\\n",
      "0  1998      1    1     0       0  0.0         0.6           241.0 -0.524772   \n",
      "1  1998      1    1     0      30  0.0         0.6           241.0 -0.524772   \n",
      "2  1998      1    1     1       0  0.0         0.6           238.0 -0.508829   \n",
      "3  1998      1    1     1      30  0.0         0.6           238.0 -0.508829   \n",
      "4  1998      1    1     2       0  0.0         0.6           225.8 -0.430146   \n",
      "\n",
      "     wind_y     day_x     day_y    time_x    time_y  max_possible_DNI  \\\n",
      "0 -0.290886  0.017213  0.999852  0.000000  1.000000               0.0   \n",
      "1 -0.290886  0.017213  0.999852  0.130526  0.991445               0.0   \n",
      "2 -0.317952  0.017213  0.999852  0.258819  0.965926               0.0   \n",
      "3 -0.317952  0.017213  0.999852  0.382683  0.923880               0.0   \n",
      "4 -0.418299  0.017213  0.999852  0.500000  0.866025               0.0   \n",
      "\n",
      "   cloudiness_factor  \n",
      "0           0.773816  \n",
      "1           0.020232  \n",
      "2           0.551991  \n",
      "3           0.019842  \n",
      "4           0.924428  \n"
     ]
    }
   ],
   "source": [
    "print(PITdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this attempt, I will use only the DNI from PGH plus the 6 closest cities (PIT, BUT, GRE, WAS, WHL, STU, ELV). The shape of this data will have each column be the DNI from a different city. The rows will be measurement instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate((PITdf['DNI'].values.reshape(-1,1), BUTdf['DNI'].values.reshape(-1,1), \\\n",
    "                           GREdf['DNI'].values.reshape(-1,1), ELVdf['DNI'].values.reshape(-1,1), \\\n",
    "                           WHLdf['DNI'].values.reshape(-1,1), STUdf['DNI'].values.reshape(-1,1), ),axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training purposes, I will use 5 years of data for training and 1 for testing. Training data will be 1998-2002 and testing data will be 2003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = 365 * 48 * 1\n",
    "train_length = 365 * 48 * 5\n",
    "train_data = all_data[:train_length,:]\n",
    "test_data = all_data[train_length:train_length + test_length, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will normalize the data between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1.],\n",
       "       ...,\n",
       "       [-1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_data_normalized = scaler.fit_transform(train_data)\n",
    "train_data_normalized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert np.array to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_normalized = torch.FloatTensor(train_data_normalized)\n",
    "test_data = torch.FloatTensor(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the previous 1 time step information from all locations to predict the DNI in PIT at timestep t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps_input = 1\n",
    "train_data_normalized_input = train_data_normalized[:-t_steps_input,:]\n",
    "train_data_normalized_targets = train_data_normalized[t_steps_input:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will attempt to build a class defining a FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feedforward(6,100)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 1.556161880493164\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(train_data_normalized_input)\n",
    "before_train = criterion(y_pred.squeeze(), train_data_normalized_targets)\n",
    "print('Test loss before training', before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 1.556161880493164\n",
      "Epoch 10: train loss: 1.1964085102081299\n",
      "Epoch 20: train loss: 1.0244988203048706\n",
      "Epoch 30: train loss: 0.9374988079071045\n",
      "Epoch 40: train loss: 0.8876590728759766\n",
      "Epoch 50: train loss: 0.8560982942581177\n",
      "Epoch 60: train loss: 0.8345780372619629\n",
      "Epoch 70: train loss: 0.8190730214118958\n",
      "Epoch 80: train loss: 0.8074215650558472\n",
      "Epoch 90: train loss: 0.7983716726303101\n",
      "Epoch 100: train loss: 0.7911536693572998\n",
      "Epoch 110: train loss: 0.7852700352668762\n",
      "Epoch 120: train loss: 0.7803866267204285\n",
      "Epoch 130: train loss: 0.7762705683708191\n",
      "Epoch 140: train loss: 0.7727553248405457\n",
      "Epoch 150: train loss: 0.7697189450263977\n",
      "Epoch 160: train loss: 0.7670698165893555\n",
      "Epoch 170: train loss: 0.764738142490387\n",
      "Epoch 180: train loss: 0.7626698017120361\n",
      "Epoch 190: train loss: 0.7608221173286438\n",
      "Epoch 200: train loss: 0.7591611742973328\n",
      "Epoch 210: train loss: 0.7576594948768616\n",
      "Epoch 220: train loss: 0.7562947273254395\n",
      "Epoch 230: train loss: 0.7550485730171204\n",
      "Epoch 240: train loss: 0.7539057731628418\n",
      "Epoch 250: train loss: 0.7528533935546875\n",
      "Epoch 260: train loss: 0.751880943775177\n",
      "Epoch 270: train loss: 0.7509790062904358\n",
      "Epoch 280: train loss: 0.7501400113105774\n",
      "Epoch 290: train loss: 0.7493571639060974\n",
      "Epoch 300: train loss: 0.7486246824264526\n",
      "Epoch 310: train loss: 0.7479374408721924\n",
      "Epoch 320: train loss: 0.7472912669181824\n",
      "Epoch 330: train loss: 0.7466822862625122\n",
      "Epoch 340: train loss: 0.7461070418357849\n",
      "Epoch 350: train loss: 0.7455626726150513\n",
      "Epoch 360: train loss: 0.7450463771820068\n",
      "Epoch 370: train loss: 0.7445561289787292\n",
      "Epoch 380: train loss: 0.7440894842147827\n",
      "Epoch 390: train loss: 0.7436448335647583\n",
      "Epoch 400: train loss: 0.7432204484939575\n",
      "Epoch 410: train loss: 0.7428147196769714\n",
      "Epoch 420: train loss: 0.7424263954162598\n",
      "Epoch 430: train loss: 0.7420541644096375\n",
      "Epoch 440: train loss: 0.7416971325874329\n",
      "Epoch 450: train loss: 0.7413540482521057\n",
      "Epoch 460: train loss: 0.7410240173339844\n",
      "Epoch 470: train loss: 0.7407063245773315\n",
      "Epoch 480: train loss: 0.7404001355171204\n",
      "Epoch 490: train loss: 0.7401047945022583\n",
      "Epoch 500: train loss: 0.7398195266723633\n",
      "Epoch 510: train loss: 0.7395439743995667\n",
      "Epoch 520: train loss: 0.7392773032188416\n",
      "Epoch 530: train loss: 0.7390192151069641\n",
      "Epoch 540: train loss: 0.7387692332267761\n",
      "Epoch 550: train loss: 0.7385268211364746\n",
      "Epoch 560: train loss: 0.7382916808128357\n",
      "Epoch 570: train loss: 0.738063395023346\n",
      "Epoch 580: train loss: 0.7378416061401367\n",
      "Epoch 590: train loss: 0.7376260161399841\n",
      "Epoch 600: train loss: 0.7374163866043091\n",
      "Epoch 610: train loss: 0.7372123599052429\n",
      "Epoch 620: train loss: 0.7370136380195618\n",
      "Epoch 630: train loss: 0.7368201613426208\n",
      "Epoch 640: train loss: 0.7366316318511963\n",
      "Epoch 650: train loss: 0.7364476919174194\n",
      "Epoch 660: train loss: 0.7362682223320007\n",
      "Epoch 670: train loss: 0.7360932230949402\n",
      "Epoch 680: train loss: 0.7359221577644348\n",
      "Epoch 690: train loss: 0.7357550859451294\n",
      "Epoch 700: train loss: 0.7355920076370239\n",
      "Epoch 710: train loss: 0.735432505607605\n",
      "Epoch 720: train loss: 0.7352764010429382\n",
      "Epoch 730: train loss: 0.7351238131523132\n",
      "Epoch 740: train loss: 0.7349745035171509\n",
      "Epoch 750: train loss: 0.7348282933235168\n",
      "Epoch 760: train loss: 0.7346851229667664\n",
      "Epoch 770: train loss: 0.7345448732376099\n",
      "Epoch 780: train loss: 0.7344075441360474\n",
      "Epoch 790: train loss: 0.7342728972434998\n",
      "Epoch 800: train loss: 0.734140932559967\n",
      "Epoch 810: train loss: 0.7340114712715149\n",
      "Epoch 820: train loss: 0.7338845133781433\n",
      "Epoch 830: train loss: 0.7337599992752075\n",
      "Epoch 840: train loss: 0.7336378693580627\n",
      "Epoch 850: train loss: 0.7335178256034851\n",
      "Epoch 860: train loss: 0.7334001064300537\n",
      "Epoch 870: train loss: 0.7332844138145447\n",
      "Epoch 880: train loss: 0.7331709265708923\n",
      "Epoch 890: train loss: 0.7330593466758728\n",
      "Epoch 900: train loss: 0.7329497933387756\n",
      "Epoch 910: train loss: 0.7328420281410217\n",
      "Epoch 920: train loss: 0.7327361106872559\n",
      "Epoch 930: train loss: 0.7326321005821228\n",
      "Epoch 940: train loss: 0.7325297594070435\n",
      "Epoch 950: train loss: 0.7324291467666626\n",
      "Epoch 960: train loss: 0.7323302030563354\n",
      "Epoch 970: train loss: 0.7322328686714172\n",
      "Epoch 980: train loss: 0.7321370840072632\n",
      "Epoch 990: train loss: 0.7320427894592285\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 100\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_pred = model(train_data_normalized_input)\n",
    "    loss = criterion(y_pred.squeeze(), train_data_normalized_targets)\n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
