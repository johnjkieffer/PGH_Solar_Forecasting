{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to start from csv files downloaded from \n",
    "https://maps.nrel.gov/nsrdb-viewer/?aL=x8CI3i%255Bv%255D%3Dt%26ozt_aP%255Bv%255D%3Dt%26ozt_aP%255Bd%255D%3D1&bL=clight&cE=0&lR=0&mC=4.740675384778373%2C22.8515625&zL=2\n",
    "for 15 different locations around Pittsburgh, PA, USA:\n",
    "Pittsburgh, PA (PIT)\n",
    "Greeensburg, PA (GRE)\n",
    "Johnstown, PA (JON)\n",
    "Morgantown, WV (MGT)\n",
    "Washington, PA (WAS)\n",
    "Wheeling, WV (WHE)\n",
    "Parkersburg, WV (PKS)\n",
    "Cambridge, OH (CBG)\n",
    "Steubenville, OH (STU)\n",
    "New Philadelphia, OH (NPH)\n",
    "East Liverpool, OH (ELV)\n",
    "Youngstown, OH (YGT)\n",
    "New Castle, OH (NCS)\n",
    "Butler, PA (BUT)\n",
    "Kittanning, PA (KIT)\n",
    "\n",
    "It assumes that 21 years worth of data are downloaded for each location. Importantly, the data includes DNI, Wind Speed, and Wind Direction for every half-hour (leap days not included in the data).\n",
    "It further assumes that the csv's are located at the following path:\n",
    "../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/\"CITY NAME_STATE ABBREVIATION/THREE LETTER CITY ABBREVIATION_YEAR.csv\n",
    "\n",
    "From these csv files, the following code stores all data in a separate dataframe for each city. It converts wind speed and wind direction into wind_x and wind_y to correct for discontinuities in wind direction around 360/0 degrees. It then converts the month and day to day_x and day_y to get rid of discontinuities at the end of each month and year. It also converts the hour and minute to time_x and time_y for similar reasons.\n",
    "Finally, it determines the maximum observed DNI for all locations at each time of year and then calculates a 'cloudiness index' based on the amount of DNI at a given time compared to the maximum DNI ever observed at that same time and place in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from datetime import date\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN THIS CODE AT THE BEGINNING OF A SESSION IN WHICH THE DATAFRAMES WILL BE USED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./PITdf.pkl','rb') as f:\n",
    "    PITdf = pickle.load(f)\n",
    "with open('./GREdf.pkl','rb') as f:\n",
    "    GREdf = pickle.load(f)\n",
    "with open('./JONdf.pkl','rb') as f:\n",
    "    JONdf = pickle.load(f)\n",
    "with open('./MGTdf.pkl','rb') as f:\n",
    "    MGTdf = pickle.load(f)\n",
    "with open('./WASdf.pkl','rb') as f:\n",
    "    WASdf = pickle.load(f)\n",
    "with open('./WHLdf.pkl','rb') as f:\n",
    "    WHLdf = pickle.load(f)\n",
    "with open('./PKSdf.pkl','rb') as f:\n",
    "    PKSdf = pickle.load(f)\n",
    "with open('./CBGdf.pkl','rb') as f:\n",
    "    CBGdf = pickle.load(f)\n",
    "with open('./STUdf.pkl','rb') as f:\n",
    "    STUdf = pickle.load(f)\n",
    "with open('./NPHdf.pkl','rb') as f:\n",
    "    NPHdf = pickle.load(f)\n",
    "with open('./ELVdf.pkl','rb') as f:\n",
    "    ELVdf = pickle.load(f)\n",
    "with open('./YGTdf.pkl','rb') as f:\n",
    "    YGTdf = pickle.load(f)\n",
    "with open('./NCSdf.pkl','rb') as f:\n",
    "    NCSdf = pickle.load(f)\n",
    "with open('./BUTdf.pkl','rb') as f:\n",
    "    BUTdf = pickle.load(f)\n",
    "with open('./KITdf.pkl','rb') as f:\n",
    "    KITdf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN THIS CODE AT THE END OF THE SESSION IN WHICH CHANGES WERE MADE TO THE DATAFRAMES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./PITdf.pkl','wb') as f:\n",
    "    pickle.dump(PITdf,f)\n",
    "with open('./GREdf.pkl','wb') as f:\n",
    "    pickle.dump(GREdf,f)\n",
    "with open('./JONdf.pkl','wb') as f:\n",
    "    pickle.dump(JONdf,f)\n",
    "with open('./MGTdf.pkl','wb') as f:\n",
    "    pickle.dump(MGTdf,f)\n",
    "with open('./WASdf.pkl','wb') as f:\n",
    "    pickle.dump(WASdf,f)\n",
    "with open('./WHLdf.pkl','wb') as f:\n",
    "    pickle.dump(WHLdf,f)\n",
    "with open('./PKSdf.pkl','wb') as f:\n",
    "    pickle.dump(PKSdf,f)\n",
    "with open('./CBGdf.pkl','wb') as f:\n",
    "    pickle.dump(CBGdf,f)\n",
    "with open('./STUdf.pkl','wb') as f:\n",
    "    pickle.dump(STUdf,f)\n",
    "with open('./NPHdf.pkl','wb') as f:\n",
    "    pickle.dump(NPHdf,f)\n",
    "with open('./ELVdf.pkl','wb') as f:\n",
    "    pickle.dump(ELVdf,f)\n",
    "with open('./YGTdf.pkl','wb') as f:\n",
    "    pickle.dump(YGTdf,f)\n",
    "with open('./NCSdf.pkl','wb') as f:\n",
    "    pickle.dump(NCSdf,f)\n",
    "with open('./BUTdf.pkl','wb') as f:\n",
    "    pickle.dump(BUTdf,f)\n",
    "with open('./KITdf.pkl','wb') as f:\n",
    "    pickle.dump(KITdf,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will get the list of cites from which data was recorded and then import all data to data frames and combine all years together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>DNI</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Wind Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>270.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>270.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>263.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>263.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>255.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367915</th>\n",
       "      <td>17515</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367916</th>\n",
       "      <td>17516</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367917</th>\n",
       "      <td>17517</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>312.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367918</th>\n",
       "      <td>17518</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367919</th>\n",
       "      <td>17519</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>308.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367920 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  Year  Month  Day  Hour  Minute  DNI  Wind Speed  Wind Direction\n",
       "0           0  1998      1    1     0       0  0.0         0.6           270.1\n",
       "1           1  1998      1    1     0      30  0.0         0.6           270.1\n",
       "2           2  1998      1    1     1       0  0.0         0.6           263.8\n",
       "3           3  1998      1    1     1      30  0.0         0.6           263.8\n",
       "4           4  1998      1    1     2       0  0.0         0.6           255.2\n",
       "...       ...   ...    ...  ...   ...     ...  ...         ...             ...\n",
       "367915  17515  2018     12   31    21      30  0.0         0.5           316.0\n",
       "367916  17516  2018     12   31    22       0  0.0         0.5           314.0\n",
       "367917  17517  2018     12   31    22      30  0.0         0.5           312.0\n",
       "367918  17518  2018     12   31    23       0  0.0         0.5           310.0\n",
       "367919  17519  2018     12   31    23      30  0.0         0.5           308.0\n",
       "\n",
       "[367920 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of cities from which data was recorded\n",
    "cities = os.listdir('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data')\n",
    "# Years during which data was recorded\n",
    "years = range(1998,2019)\n",
    "# Create data frames for each city by appending all the years together. \n",
    "PITdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Pittsburgh, PA/PIT_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t PITdf = PITdf.append(tempdf)\n",
    "cols = PITdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        PITdf[col] = PITdf[col].astype(int)\n",
    "    else:\n",
    "        PITdf[col] = PITdf[col].astype(float)\n",
    "PITdf.reset_index()\n",
    "\n",
    "\n",
    "GREdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Greensburg, PA/GRE_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t GREdf = GREdf.append(tempdf)\n",
    "cols = GREdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        GREdf[col] = GREdf[col].astype(int)\n",
    "    else:\n",
    "        GREdf[col] = GREdf[col].astype(float)\n",
    "GREdf.reset_index()\n",
    "\n",
    "JONdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Johnstown, PA/JON_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t JONdf = JONdf.append(tempdf)\n",
    "cols = JONdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        JONdf[col] = JONdf[col].astype(int)\n",
    "    else:\n",
    "        JONdf[col] = JONdf[col].astype(float)\n",
    "JONdf.reset_index()\n",
    "\n",
    "\n",
    "MGTdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Morgantown, WV/MGT_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t MGTdf = MGTdf.append(tempdf)\n",
    "cols = MGTdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        MGTdf[col] = MGTdf[col].astype(int)\n",
    "    else:\n",
    "        MGTdf[col] = MGTdf[col].astype(float)\n",
    "MGTdf.reset_index()\n",
    "\n",
    "\n",
    "WASdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Washington, PA/WAS_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t WASdf = WASdf.append(tempdf)\n",
    "cols = WASdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        WASdf[col] = WASdf[col].astype(int)\n",
    "    else:\n",
    "        WASdf[col] = WASdf[col].astype(float)\n",
    "WASdf.reset_index()\n",
    "\n",
    "\n",
    "WHLdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Wheeling, WV/WHL_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t WHLdf = WHLdf.append(tempdf)\n",
    "cols = WHLdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        WHLdf[col] = WHLdf[col].astype(int)\n",
    "    else:\n",
    "        WHLdf[col] = WHLdf[col].astype(float)\n",
    "WHLdf.reset_index()\n",
    "\n",
    "\n",
    "PKSdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Parkersburg, WV/PKS_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t PKSdf = PKSdf.append(tempdf)\n",
    "cols = PKSdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        PKSdf[col] = PKSdf[col].astype(int)\n",
    "    else:\n",
    "        PKSdf[col] = PKSdf[col].astype(float)\n",
    "PKSdf.reset_index()\n",
    "\n",
    "\n",
    "CBGdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Cambridge, OH/CBG_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t CBGdf = CBGdf.append(tempdf)\n",
    "cols = CBGdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        CBGdf[col] = CBGdf[col].astype(int)\n",
    "    else:\n",
    "        CBGdf[col] = CBGdf[col].astype(float)\n",
    "CBGdf.reset_index()\n",
    "\n",
    "\n",
    "STUdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Steubenville, OH/STU_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t STUdf = STUdf.append(tempdf)\n",
    "cols = STUdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        STUdf[col] = STUdf[col].astype(int)\n",
    "    else:\n",
    "        STUdf[col] = STUdf[col].astype(float)\n",
    "STUdf.reset_index()\n",
    "\n",
    "\n",
    "NPHdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/New_Philadelphia, OH/NPH_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t NPHdf = NPHdf.append(tempdf)\n",
    "cols = NPHdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        NPHdf[col] = NPHdf[col].astype(int)\n",
    "    else:\n",
    "        NPHdf[col] = NPHdf[col].astype(float)\n",
    "NPHdf.reset_index()\n",
    "\n",
    "\n",
    "ELVdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/East_Liverpool, OH/ELV_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t ELVdf = ELVdf.append(tempdf)\n",
    "cols = ELVdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        ELVdf[col] = ELVdf[col].astype(int)\n",
    "    else:\n",
    "        ELVdf[col] = ELVdf[col].astype(float)\n",
    "ELVdf.reset_index()\n",
    "\n",
    "\n",
    "YGTdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Youngstown, OH/YGT_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t YGTdf = YGTdf.append(tempdf)\n",
    "cols = YGTdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        YGTdf[col] = YGTdf[col].astype(int)\n",
    "    else:\n",
    "        YGTdf[col] = YGTdf[col].astype(float)\n",
    "YGTdf.reset_index()\n",
    "\n",
    "\n",
    "NCSdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/New_Castle, PA/NCS_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t NCSdf = NCSdf.append(tempdf)\n",
    "cols = NCSdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        NCSdf[col] = NCSdf[col].astype(int)\n",
    "    else:\n",
    "        NCSdf[col] = NCSdf[col].astype(float)\n",
    "NCSdf.reset_index()\n",
    "\n",
    "\n",
    "BUTdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Butler, PA/BUT_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t BUTdf = BUTdf.append(tempdf)\n",
    "cols = BUTdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        BUTdf[col] = BUTdf[col].astype(int)\n",
    "    else:\n",
    "        BUTdf[col] = BUTdf[col].astype(float)\n",
    "BUTdf.reset_index()\n",
    "\n",
    "\n",
    "KITdf = pd.DataFrame(columns = ['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction'])\n",
    "for i in years:\n",
    "\t temp = pd.read_csv('../PGH_Solar_Forecasting/Solar_Wind_and_Cloud_Data/Kittanning, PA/KIT_' + str(i) + '.csv', header = 2)\n",
    "\t tempdf = temp[['Year', 'Month','Day','Hour','Minute','DNI','Wind Speed','Wind Direction']]\n",
    "\t KITdf = KITdf.append(tempdf)\n",
    "cols = KITdf.columns\n",
    "for col in cols:\n",
    "    if col in (\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"): \n",
    "        KITdf[col] = KITdf[col].astype(int)\n",
    "    else:\n",
    "        KITdf[col] = KITdf[col].astype(float)\n",
    "KITdf.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will try to create a new column in each data frame called 'wind_x' which will be the x componenet of the wind direction (to solve the issue of a discontinuity in this data around 360/0 degrees).\n",
    "\n",
    "I will also multiply this value by the wind speed. (I could later normalize/scale this differently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PITdf['wind_x'] = PITdf['Wind Speed'] * np.sin(np.radians(PITdf['Wind Direction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Month  Day  Hour  Minute  DNI  Wind Speed  Wind Direction    wind_x\n",
      "0  1998      1    1     0       0  0.0         0.6           241.0 -0.524772\n",
      "1  1998      1    1     0      30  0.0         0.6           241.0 -0.524772\n",
      "2  1998      1    1     1       0  0.0         0.6           238.0 -0.508829\n",
      "3  1998      1    1     1      30  0.0         0.6           238.0 -0.508829\n",
      "4  1998      1    1     2       0  0.0         0.6           225.8 -0.430146\n"
     ]
    }
   ],
   "source": [
    "print(PITdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will add a new column called 'wind_y' which will be the y component of the wind direction and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PITdf['wind_y'] = PITdf['Wind Speed'] * np.cos(np.radians(PITdf['Wind Direction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367920\n",
      "   Year  Month  Day  Hour  Minute  DNI  Wind Speed  Wind Direction    wind_x  \\\n",
      "0  1998      1    1     0       0  0.0         0.6           241.0 -0.524772   \n",
      "1  1998      1    1     0      30  0.0         0.6           241.0 -0.524772   \n",
      "2  1998      1    1     1       0  0.0         0.6           238.0 -0.508829   \n",
      "3  1998      1    1     1      30  0.0         0.6           238.0 -0.508829   \n",
      "4  1998      1    1     2       0  0.0         0.6           225.8 -0.430146   \n",
      "\n",
      "     wind_y  \n",
      "0 -0.290886  \n",
      "1 -0.290886  \n",
      "2 -0.317952  \n",
      "3 -0.317952  \n",
      "4 -0.418299  \n"
     ]
    }
   ],
   "source": [
    "print(len(PITdf))\n",
    "print(PITdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will do this same process for all data frames (for each city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREdf['wind_x'] = GREdf['Wind Speed'] * np.sin(np.radians(GREdf['Wind Direction']))\n",
    "GREdf['wind_y'] = GREdf['Wind Speed'] * np.cos(np.radians(GREdf['Wind Direction']))\n",
    "\n",
    "JONdf['wind_x'] = JONdf['Wind Speed'] * np.sin(np.radians(JONdf['Wind Direction']))\n",
    "JONdf['wind_y'] = JONdf['Wind Speed'] * np.cos(np.radians(JONdf['Wind Direction']))\n",
    "\n",
    "MGTdf['wind_x'] = MGTdf['Wind Speed'] * np.sin(np.radians(MGTdf['Wind Direction']))\n",
    "MGTdf['wind_y'] = MGTdf['Wind Speed'] * np.cos(np.radians(MGTdf['Wind Direction']))\n",
    "\n",
    "WASdf['wind_x'] = WASdf['Wind Speed'] * np.sin(np.radians(WASdf['Wind Direction']))\n",
    "WASdf['wind_y'] = WASdf['Wind Speed'] * np.cos(np.radians(WASdf['Wind Direction']))\n",
    "\n",
    "WHLdf['wind_x'] = WHLdf['Wind Speed'] * np.sin(np.radians(WHLdf['Wind Direction']))\n",
    "WHLdf['wind_y'] = WHLdf['Wind Speed'] * np.cos(np.radians(WHLdf['Wind Direction']))\n",
    "\n",
    "PKSdf['wind_x'] = PKSdf['Wind Speed'] * np.sin(np.radians(PKSdf['Wind Direction']))\n",
    "PKSdf['wind_y'] = PKSdf['Wind Speed'] * np.cos(np.radians(PKSdf['Wind Direction']))\n",
    "\n",
    "CBGdf['wind_x'] = CBGdf['Wind Speed'] * np.sin(np.radians(CBGdf['Wind Direction']))\n",
    "CBGdf['wind_y'] = CBGdf['Wind Speed'] * np.cos(np.radians(CBGdf['Wind Direction']))\n",
    "\n",
    "STUdf['wind_x'] = STUdf['Wind Speed'] * np.sin(np.radians(STUdf['Wind Direction']))\n",
    "STUdf['wind_y'] = STUdf['Wind Speed'] * np.cos(np.radians(STUdf['Wind Direction']))\n",
    "\n",
    "NPHdf['wind_x'] = NPHdf['Wind Speed'] * np.sin(np.radians(NPHdf['Wind Direction']))\n",
    "NPHdf['wind_y'] = NPHdf['Wind Speed'] * np.cos(np.radians(NPHdf['Wind Direction']))\n",
    "\n",
    "ELVdf['wind_x'] = ELVdf['Wind Speed'] * np.sin(np.radians(ELVdf['Wind Direction']))\n",
    "ELVdf['wind_y'] = ELVdf['Wind Speed'] * np.cos(np.radians(ELVdf['Wind Direction']))\n",
    "\n",
    "YGTdf['wind_x'] = YGTdf['Wind Speed'] * np.sin(np.radians(YGTdf['Wind Direction']))\n",
    "YGTdf['wind_y'] = YGTdf['Wind Speed'] * np.cos(np.radians(YGTdf['Wind Direction']))\n",
    "\n",
    "NCSdf['wind_x'] = NCSdf['Wind Speed'] * np.sin(np.radians(NCSdf['Wind Direction']))\n",
    "NCSdf['wind_y'] = NCSdf['Wind Speed'] * np.cos(np.radians(NCSdf['Wind Direction']))\n",
    "\n",
    "BUTdf['wind_x'] = BUTdf['Wind Speed'] * np.sin(np.radians(BUTdf['Wind Direction']))\n",
    "BUTdf['wind_y'] = BUTdf['Wind Speed'] * np.cos(np.radians(BUTdf['Wind Direction']))\n",
    "\n",
    "KITdf['wind_x'] = KITdf['Wind Speed'] * np.sin(np.radians(KITdf['Wind Direction']))\n",
    "KITdf['wind_y'] = KITdf['Wind Speed'] * np.cos(np.radians(KITdf['Wind Direction']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will split the 'day' and 'month' data into 'day_x' and 'day_y' to avoid the discontinuties at the end/beginning of each month and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_to_x(row):\n",
    "    yr = int(row['Year'])\n",
    "    mo = int(row['Month'])\n",
    "    da = int(row['Day'])\n",
    "    d = date(yr, mo, da)\n",
    "    first_of_year = date(yr, 1, 1)\n",
    "    diff = d - first_of_year\n",
    "    day_of_year = diff.days + 1\n",
    "    percent = day_of_year / 365\n",
    "    rad = 2 * math.pi * percent\n",
    "    d_x = np.sin(rad)\n",
    "    return d_x\n",
    "\n",
    "def day_to_y(row):\n",
    "    yr = int(row['Year'])\n",
    "    mo = int(row['Month'])\n",
    "    da = int(row['Day'])\n",
    "    d = date(yr, mo, da)\n",
    "    first_of_year = date(yr, 1, 1)\n",
    "    diff = d - first_of_year\n",
    "    day_of_year = diff.days + 1\n",
    "    percent = day_of_year / 365\n",
    "    rad = 2 * math.pi * percent\n",
    "    d_y = np.cos(rad)\n",
    "    return d_y\n",
    "\n",
    "\n",
    "\n",
    "PITdf['day_x'] = PITdf.apply(day_to_x, axis = 1)\n",
    "PITdf['day_y'] = PITdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "GREdf['day_x'] = GREdf.apply(day_to_x, axis = 1)\n",
    "GREdf['day_y'] = GREdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "JONdf['day_x'] = JONdf.apply(day_to_x, axis = 1)\n",
    "JONdf['day_y'] = JONdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "MGTdf['day_x'] = MGTdf.apply(day_to_x, axis = 1)\n",
    "MGTdf['day_y'] = MGTdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "WASdf['day_x'] = WASdf.apply(day_to_x, axis = 1)\n",
    "WASdf['day_y'] = WASdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "WHLdf['day_x'] = WHLdf.apply(day_to_x, axis = 1)\n",
    "WHLdf['day_y'] = WHLdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "PKSdf['day_x'] = PKSdf.apply(day_to_x, axis = 1)\n",
    "PKSdf['day_y'] = PKSdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "CBGdf['day_x'] = CBGdf.apply(day_to_x, axis = 1)\n",
    "CBGdf['day_y'] = CBGdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "STUdf['day_x'] = STUdf.apply(day_to_x, axis = 1)\n",
    "STUdf['day_y'] = STUdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "NPHdf['day_x'] = NPHdf.apply(day_to_x, axis = 1)\n",
    "NPHdf['day_y'] = NPHdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "ELVdf['day_x'] = ELVdf.apply(day_to_x, axis = 1)\n",
    "ELVdf['day_y'] = ELVdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "YGTdf['day_x'] = YGTdf.apply(day_to_x, axis = 1)\n",
    "YGTdf['day_y'] = YGTdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "NCSdf['day_x'] = NCSdf.apply(day_to_x, axis = 1)\n",
    "NCSdf['day_y'] = NCSdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "BUTdf['day_x'] = BUTdf.apply(day_to_x, axis = 1)\n",
    "BUTdf['day_y'] = BUTdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "KITdf['day_x'] = KITdf.apply(day_to_x, axis = 1)\n",
    "KITdf['day_y'] = KITdf.apply(day_to_y, axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, split Hour and Minute into time_x and time_y and decide how to normalize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_x(row):\n",
    "    hr = int(row['Hour'])\n",
    "    m = int(row['Minute'])\n",
    "    time = hr*60 + m\n",
    "    percent_of_day = time/1440\n",
    "    rad = 2 * math.pi * percent_of_day\n",
    "    t_x = np.sin(rad)\n",
    "    return t_x\n",
    "    \n",
    "\n",
    "def time_to_y(row):\n",
    "    hr = int(row['Hour'])\n",
    "    m = int(row['Minute'])\n",
    "    time = hr*60 + m\n",
    "    percent_of_day = time/1440\n",
    "    rad = 2 * math.pi * percent_of_day\n",
    "    t_y = np.cos(rad)\n",
    "    return t_y\n",
    "    \n",
    "\n",
    "\n",
    "PITdf['time_x'] = PITdf.apply(time_to_x, axis = 1)\n",
    "PITdf['time_y'] = PITdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "GREdf['time_x'] = GREdf.apply(time_to_x, axis = 1)\n",
    "GREdf['time_y'] = GREdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "JONdf['time_x'] = JONdf.apply(time_to_x, axis = 1)\n",
    "JONdf['time_y'] = JONdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "MGTdf['time_x'] = MGTdf.apply(time_to_x, axis = 1)\n",
    "MGTdf['time_y'] = MGTdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "WASdf['time_x'] = WASdf.apply(time_to_x, axis = 1)\n",
    "WASdf['time_y'] = WASdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "WHLdf['time_x'] = WHLdf.apply(time_to_x, axis = 1)\n",
    "WHLdf['time_y'] = WHLdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "PKSdf['time_x'] = PKSdf.apply(time_to_x, axis = 1)\n",
    "PKSdf['time_y'] = PKSdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "CBGdf['time_x'] = CBGdf.apply(time_to_x, axis = 1)\n",
    "CBGdf['time_y'] = CBGdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "STUdf['time_x'] = STUdf.apply(time_to_x, axis = 1)\n",
    "STUdf['time_y'] = STUdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "NPHdf['time_x'] = NPHdf.apply(time_to_x, axis = 1)\n",
    "NPHdf['time_y'] = NPHdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "ELVdf['time_x'] = ELVdf.apply(time_to_x, axis = 1)\n",
    "ELVdf['time_y'] = ELVdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "YGTdf['time_x'] = YGTdf.apply(time_to_x, axis = 1)\n",
    "YGTdf['time_y'] = YGTdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "NCSdf['time_x'] = NCSdf.apply(time_to_x, axis = 1)\n",
    "NCSdf['time_y'] = NCSdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "BUTdf['time_x'] = BUTdf.apply(time_to_x, axis = 1)\n",
    "BUTdf['time_y'] = BUTdf.apply(time_to_y, axis = 1)\n",
    "\n",
    "KITdf['time_x'] = KITdf.apply(time_to_x, axis = 1)\n",
    "KITdf['time_y'] = KITdf.apply(time_to_y, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, find the max possible DNI at all times of the year (based on the 21 years of data I have), and add this to the dataframes for each city.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = PITdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "PITdf['max_possible_DNI'] = final\n",
    "PITdf = PITdf.reset_index(drop=True)\n",
    "\n",
    "temp = GREdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "GREdf['max_possible_DNI'] = final\n",
    "GREdf = GREdf.reset_index(drop=True)\n",
    "\n",
    "temp = JONdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "JONdf['max_possible_DNI'] = final\n",
    "JONdf = JONdf.reset_index(drop=True)\n",
    "\n",
    "temp = MGTdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "MGTdf['max_possible_DNI'] = final\n",
    "MGTdf = MGTdf.reset_index(drop=True)\n",
    "\n",
    "temp = WASdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "WASdf['max_possible_DNI'] = final\n",
    "WASdf = WASdf.reset_index(drop=True)\n",
    "\n",
    "temp = WHLdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "WHLdf['max_possible_DNI'] = final\n",
    "WHLdf = WHLdf.reset_index(drop=True)\n",
    "\n",
    "temp = PKSdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "PKSdf['max_possible_DNI'] = final\n",
    "PKSdf = PKSdf.reset_index(drop=True)\n",
    "\n",
    "temp = CBGdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "CBGdf['max_possible_DNI'] = final\n",
    "CBGdf = CBGdf.reset_index(drop=True)\n",
    "\n",
    "temp = STUdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "STUdf['max_possible_DNI'] = final\n",
    "STUdf = STUdf.reset_index(drop=True)\n",
    "\n",
    "temp = NPHdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "NPHdf['max_possible_DNI'] = final\n",
    "NPHdf = NPHdf.reset_index(drop=True)\n",
    "\n",
    "temp = ELVdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "ELVdf['max_possible_DNI'] = final\n",
    "ELVdf = ELVdf.reset_index(drop=True)\n",
    "\n",
    "temp = YGTdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "YGTdf['max_possible_DNI'] = final\n",
    "YGTdf = YGTdf.reset_index(drop=True)\n",
    "\n",
    "temp = NCSdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "NCSdf['max_possible_DNI'] = final\n",
    "NCSdf = NCSdf.reset_index(drop=True)\n",
    "\n",
    "temp = BUTdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "BUTdf['max_possible_DNI'] = final\n",
    "BUTdf = BUTdf.reset_index(drop=True)\n",
    "\n",
    "temp = KITdf.groupby(['Month','Day','Hour','Minute']).agg({'DNI': 'max'})\n",
    "temp.columns = ['max_possible_DNI']\n",
    "temp = temp.reset_index()\n",
    "temp1 = temp['max_possible_DNI']\n",
    "final = pd.concat([temp1 for i in range(21)], ignore_index = True)\n",
    "KITdf['max_possible_DNI'] = final\n",
    "KITdf = KITdf.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide all DNI values by the corresponding max DNI value and subtract from 1 to get a cloudiness variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloudiness_factor(row):\n",
    "    if row['max_possible_DNI'] == 0:\n",
    "        x = random.random()\n",
    "        return x\n",
    "    else:\n",
    "        c_f = 1 - (row['DNI'] / row['max_possible_DNI'])\n",
    "        return c_f\n",
    "\n",
    "PITdf['cloudiness_factor'] = PITdf.apply(cloudiness_factor, axis = 1)\n",
    "GREdf['cloudiness_factor'] = GREdf.apply(cloudiness_factor, axis = 1)\n",
    "JONdf['cloudiness_factor'] = JONdf.apply(cloudiness_factor, axis = 1)\n",
    "MGTdf['cloudiness_factor'] = MGTdf.apply(cloudiness_factor, axis = 1)\n",
    "WASdf['cloudiness_factor'] = WASdf.apply(cloudiness_factor, axis = 1)\n",
    "WHLdf['cloudiness_factor'] = WHLdf.apply(cloudiness_factor, axis = 1)\n",
    "PKSdf['cloudiness_factor'] = PKSdf.apply(cloudiness_factor, axis = 1)\n",
    "CBGdf['cloudiness_factor'] = CBGdf.apply(cloudiness_factor, axis = 1)\n",
    "STUdf['cloudiness_factor'] = STUdf.apply(cloudiness_factor, axis = 1)\n",
    "NPHdf['cloudiness_factor'] = NPHdf.apply(cloudiness_factor, axis = 1)\n",
    "ELVdf['cloudiness_factor'] = ELVdf.apply(cloudiness_factor, axis = 1)\n",
    "YGTdf['cloudiness_factor'] = YGTdf.apply(cloudiness_factor, axis = 1)\n",
    "NCSdf['cloudiness_factor'] = NCSdf.apply(cloudiness_factor, axis = 1)\n",
    "BUTdf['cloudiness_factor'] = BUTdf.apply(cloudiness_factor, axis = 1)\n",
    "KITdf['cloudiness_factor'] = KITdf.apply(cloudiness_factor, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
